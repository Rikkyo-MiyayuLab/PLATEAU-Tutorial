# チュートリアル① モデル分析

## 目次

## 学習過程の分析

`Tensorboardコマンド`を使用して、訓練したモデルの学習過程を可視化することができます。
下記のコマンドを実行して、学習過程を確認してみましょう。

```bash
$ tensorboard --logdir <モデルの保存先>
```

するとターミナルに下記のようなメッセージが表示されるので、表示されているURLをブラウザで開きます。
```
TensorFlow installation not found - running with reduced feature set.
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)
```

ブラウザで表示された画面には、下図のような学習過程のグラフが表示されます。
![alt text](image.png)

それぞれのグラフの説明については、割愛いたしますが、学習結果の成否を判断する上で、以下の累積報酬のグラフが重要な為、これについて少し説明します。

#### 累積報酬の推移
![alt text](image-1.png)

最も重要なグラフが、この累積報酬の推移です。
このグラフは、エージェントが環境とやり取りする中で得た報酬の合計を示しています。

強化学習はエージェントが環境の状態を観測し、行動を出力、その行動の結果として得られる報酬を最大化するように学習を進めます。

そのため、学習初期段階から比較して、学習が進むにつれて累積報酬が増加していることが望ましいです。
報酬がある一定程度の値まで増加し、収束している場合は、エージェントは何らかの法則に基づいて、安定した報酬が得られるような行動の出力を学習できたと言えます。

<details>
<summary>今回作成したモデルについての考察</summary>
今回作成したモデルの累積報酬は、学習初期段階では低い値を示しているものの、学習の進行に伴い右肩上がりになっていることが確認できます。
学習回数が20回（今回のシミュレーションでは20step目）あたりまで増加傾向を続け、その後は大きく減少することなく、横這いとなり、シミュレーション結果が安定していることを示しています。

この結果から、今回作成したモデルは、学習回数が20回（今回のシミュレーションでは20step目）あたりまでに、観測した情報から、安定した報酬（避難率）を維持するための何らかの法則を掴み、その結果に基づいて、避難所の選択を行ったと考えられます。
</details>

## モデルの評価

今回作成したモデルをランダムに建物を選択する場合（モデル無し）と比較し、最終的な避難率の結果を確認しモデルの性能を評価してみましょう。

### データ記録用の処理の作成

### データ可視化用の処理の作成

### 実験① モデル無しの場合

### 実験② モデル有りの場合

### モデルの行動分析
<!--エージェントの行動ログを分析し、選択した建物の回数分布をみてみる-->



